**Summary of “Capable but Not Cooperative? Perceptions of ChatGPT as a Pragmatic Speaker”**

This paper sits at the intersection of pragmatics and human–computer interaction. It asks whether people assume that ChatGPT, a large language model, will follow conversational maxims in the same way they expect adult humans to do. In other words, even if users believe ChatGPT is highly competent, do they still draw pragmatic inferences—such as Gricean implicatures—when interpreting its messages?

The **key research questions** are:  
1. When ChatGPT generates an ambiguous referring expression (e.g. “red” to refer to one of two red shapes), do people interpret it as cooperatively as they do when an adult human speaker produces it?  
2. How do these interpretations compare to those elicited when participants are told the speaker is a 4-year-old child?  
3. Finally, do users’ explicit beliefs about ChatGPT’s reasoning ability (measured in a post-test questionnaire) match their actual interpretations in the task?

To answer these questions, the authors adapt a **reference-game paradigm** from Mayn and Demberg (2024). Participants see displays with three shapes that vary in color and form (e.g. red square, red circle, blue triangle). On each trial, they are told ChatGPT chose one of four possible messages (only “circle,” “triangle,” “red,” or “green”) to refer to a highlighted target. Participants then allocate 100 points across the three objects according to how likely they think ChatGPT intended each one. Eight trials are **critical**—where a literal color or shape label is ambiguous, but a cooperative speaker should choose the unambiguous alternative (e.g. “circle”)—and sixteen are **controls** (unambiguous or genuinely ambiguous). The study collects 40 native-English participants on Prolific, excludes those who misunderstand, and compares their responses to existing data for adult and child speaker conditions.

**Main findings** show that, on critical trials, users assign lower probabilities to the intended target when they believe ChatGPT is the speaker (mean ≈ 60%) than when they believe it’s an adult human (≈ 71%), but slightly higher than when it’s a child (≈ 57%). A mixed-effects analysis confirms significant differences: ChatGPT is seen as less cooperative than an adult but slightly more so than a child. Annotation of participants’ open-ended explanations reveals that fewer users invoke alternative-based reasoning for ChatGPT than for an adult speaker.  

Interestingly, in a **post-test questionnaire**, most participants rate ChatGPT’s ability to reason about alternatives very highly (average 4.05/5), even when their point allocations suggest they did not actually apply that reasoning. This disconnect suggests that, although users believe in ChatGPT’s competence, they may not spontaneously assume it will behave as cooperatively as an adult human interlocutor in pragmatic tasks.